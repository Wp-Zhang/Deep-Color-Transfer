[{"content":"","date":"23 October 2022","permalink":"/Deep-Color-Transfer/","section":"Deep Color Transfer","summary":"","title":"Deep Color Transfer"},{"content":"","date":"23 October 2022","permalink":"/Deep-Color-Transfer/posts/","section":"Posts","summary":"","title":"Posts"},{"content":" Conceptual Review # The authors of Deep Color Transfer Using Histogram Analogy [1] proposed a deep learning framework that leverages color histogram analogy for color transfer between source and reference images. The framework consists of two networks, a Histogram Encoding Network (HEN) and a Color Transfer Network (CTN) (Fig. 1). HEN extracts encoded information from the histograms of source and reference images, which is fed into CTN to guide the color transfer process. Although a histogram is a simple and global representation of image colors, convolutional neural networks with encoded histograms can conditionally transfer the colors of reference images to source images. For strongly relevant and irrelevant cases, the same histogram information is used for all parts of the source image, and this is the default setting. When semantic object information is important, as in the case of weak relevance, semantic image segmentation is used and the histogram analogy is extracted and applied for corresponding semantic regions between source and reference images.\nFig. 1 Network architecture. Implementation Details # As the authors did in [1] , we’ll train our model on a paired dataset constructed from the MIT-Adobe 5K dataset [2] which consists of six sets, each of which contains 5,000 images. Since using the dataset only provides image pairs with a fixed number of combinations, we also follow the authors to perform color augmentation by transforming the average hue and saturation of the original images to produce more diverse image pairs. In addition, as an ideally trained model must produce the output image the same as the source image if the histograms of the two images are the same, we used identical source and reference image pairs to stabilize the network output. We implement our network on Pytorch. We use the Adam optimizer with the fixed learning rate \\(5*10^{-5}\\), \\(\\beta_1=0.5\\), and \\(\\beta_2=0.999\\). As we jointly train HEN and CTN, the objective function is \\(L_{total} = L_{image} + \\lambda_1 L_{hist} + \\lambda_2 L_{multi}\\), where \\(L_{image}\\), \\(L_{hist}\\) and \\(L_{multi}\\) are a image loss, a histogram loss, and a multi-scale loss. We used \\(\\lambda_1=1.5\\) and \\(\\lambda_2=0.5\\). We train the network for 100 epochs.\nFindings # As the image semantic segmentation map may have a huge impact on the final output of our model, we tried several segmentaion models and finalized BEiT v2 [3] as our segmentation model. We built the model and loaded downloaded pre-trained weights and then generated the segmentation maps. As shown in Fig 2, the segmentation result from BEiT v2 is quite accurate.\nFig. 2 BEiT v2 generated segmentation map. With accurate segmentation maps generated, we tested 4 sets of image pairs and the result is showed in Fig 3. When the reference image and input image have similar semantic content, the model can generate a great color transfer without being fed with segmentation maps. However, when there\u0026rsquo;s a huge content difference, segmentation maps can help improve the model performance (see row 3). The model works well even when the reference image is just a color palette.\nFig. 3 Experiment results on different Plans # The experiment we did above meets our expection. In the next few weeks, we\u0026rsquo;ll try to change the structure of our current model and see if better results can be generated. The size of our model is relatively large right now, we\u0026rsquo;ll also try some knowledge distillation techniques and smaller model structure like MobileNet.\n[1] Lee, Junyong, et al. \u0026ldquo;Deep color transfer using histogram analogy.\u0026rdquo; The Visual Computer 36.10 (2020): 2129-2143. # [2] Bychkovsky, Vladimir, et al. \u0026ldquo;Learning photographic global tonal adjustment with a database of input/output image pairs.\u0026rdquo; CVPR 2011. IEEE, 2011. # [3] Peng, Zhiliang, et al. \u0026ldquo;Beit v2: Masked image modeling with vector-quantized visual tokenizers.\u0026rdquo; arXiv preprint arXiv:2208.06366 (2022). # ","date":"23 October 2022","permalink":"/Deep-Color-Transfer/posts/progress/","section":"Posts","summary":"Conceptual Review # The authors of Deep Color Transfer Using Histogram Analogy [1] proposed a deep learning framework that leverages color histogram analogy for color transfer between source and reference images.","title":"Progress Report"},{"content":"In this project, we are trying to build a deep learning model for color transfer between images.\nEarly color transfer approaches change colors of a source image based on the global color distribution of a reference image. Some methods perform color transfer by matching the means and standard deviations of source to reference images while others explore color mapping between source and reference images leveraging probability density functions of colors. These conventional approaches, however, may fail to reflect semantic correspondence because they do not consider any spatial information. To handle the limitation of global color transfer, several approaches contemplate local correspondences between source and reference images. While these approaches solve the problem of global color transfer in the case that source and reference images have similar structures, they may fail when both images have totally irrelevant contents and styles.\nFig. 1 Demo of Deep Color Transfer Using Histogram Analogy. The authors of Deep Color Transfer Using Histogram Analogy [1] proposed a deep learning framework that leverages color histogram analogy for color transfer between source and reference images (Fig. 1). The framework consists of two networks, a Histogram Encoding Network (HEN) and a Color Transfer Network (CTN) (Fig. 2). HEN extracts encoded information from the histograms of source and reference images, which is fed into CTN to guide the color transfer process. Although a histogram is a simple and global representation of image colors, convolutional neural networks with encoded histograms can conditionally transfer the colors of reference images to source images. For strongly relevant and irrelevant cases, the same histogram information is used for all parts of the source image, and this is the default setting. When semantic object information is important, as in the case of weak relevance, semantic image segmentation is used and the histogram analogy is extracted and applied for corresponding semantic regions between source and reference images.\nFig. 2 Network architecture. As the authors did in [1] , we’ll train our model on a paired dataset constructed from the MIT-Adobe 5K dataset [2] which consists of six sets, each of which contains 5,000 images. In the dataset, the first set contains the original images and the other five sets contain color varied images of the original images retouched by five different experts. The paired dataset enables the model to learn to transfer color into natural-looking images. Since using the dataset only provides image pairs with a fixed number of combinations, we’ll also perform color augmentation by transforming the average hue and saturation of the original images to produce more diverse image pairs. In addition, as an ideally trained model must produce the output image the same as the source image if the histograms of the two images are the same, we’ll use identical source and reference image pairs to stabilize the network output.\nThe paper this project bases on was published in 2020 and it has been 2 years since that, during which many advanced vision models were proposed and have brought the scores of benchmarks to a new level. Beyond implementing the model mentioned in [1] , we will try some SOTA semantic(or panoptic) segmentation methods to see if they could improve the performance in the case of weak relevance between the source image and the reference image. Image transformers like BEiT [3] , Swin Transformers [4] , and MaX-DeepLab [5] have drawn much attention recently, we will try to introduce transformers into the current framework and see if a better color transfer can be produced.\n[1] Lee, Junyong, et al. \u0026ldquo;Deep color transfer using histogram analogy.\u0026rdquo; The Visual Computer 36.10 (2020): 2129-2143. # [2] Bychkovsky, Vladimir, et al. \u0026ldquo;Learning photographic global tonal adjustment with a database of input/output image pairs.\u0026rdquo; CVPR 2011. IEEE, 2011. # [3] Bao, Hangbo, Li Dong, and Furu Wei. \u0026ldquo;Beit: Bert pre-training of image transformers.\u0026rdquo; arXiv preprint arXiv:2106.08254 (2021). # [4] Liu, Ze, et al. \u0026ldquo;Swin transformer: Hierarchical vision transformer using shifted windows.\u0026rdquo; Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. # [5] Wang, Huiyu, et al. \u0026ldquo;Max-deeplab: End-to-end panoptic segmentation with mask transformers.\u0026rdquo; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. # ","date":"18 October 2022","permalink":"/Deep-Color-Transfer/posts/proposal/","section":"Posts","summary":"In this project, we are trying to build a deep learning model for color transfer between images.\nEarly color transfer approaches change colors of a source image based on the global color distribution of a reference image.","title":"Project Proposal"},{"content":"","date":"1 January 0001","permalink":"/Deep-Color-Transfer/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/Deep-Color-Transfer/tags/","section":"Tags","summary":"","title":"Tags"}]