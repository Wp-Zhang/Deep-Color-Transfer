[{"content":"","date":"5 December 2022","permalink":"/Deep-Color-Transfer/","section":"Deep Color Transfer","summary":"","title":"Deep Color Transfer"},{"content":" Deep color transfer is an approach to extracting the histogram containing color information from the source and the reference image, then transferring the color between the image pair based on the histogram using a deep neural network. Besides the source image and reference image, semantic segmentation results of both images are optional inputs that can help produce a more precise color transfer. The method we adopted was published in 2020. Even though it can produce excellent color transfer effects, the size of the model is too large and models that can produce more precise semantic segmentation results have been proposed. We used BEiT v2 to generate more precise semantic segmentation results and a Color Transfer Network with fewer parameters to improve model efficiency. We leveraged knowledge distillation in training the new Color Transfer Network. Experimental results showed that our method could generate satisfying color transfer effects faster and support larger input images.\n1. Introduction\u003e 1. Introduction # Color is the most direct visual perception of an image and the central consideration when people edit their photographs. Color filters are commonly seen in the applications such as Instagram and TikTok. Color transfer in our project refers to adequately transferring the reference image\u0026rsquo;s color to the source image while keeping the objects in the source images unchanged.\nFigure 1: Example of Color transfer result Our main contributions are summarized as follows:\nIntroduced BEiT v2 [1] to the histogram encode network and a Color Transfer Network with fewer parameters. Applied knowledge distillation [2] during the training of the smaller model, significantly reducing the time of training and enabling the model to produce similar color transfer effects to the original model while increasing the inference speed and maximum resolution of input images. 2. Related work\u003e 2. Related work # Color transfer The Color transfer method is constantly evolving. The conventional approaches try to match the means and standard deviations of the source to reference images or calculate probability density functions. Other solutions are also based on the local color part, including the EM algorithm and neural representations. Lee et al. [3] adopted the idea from photorealistic image style transfer [4] and introduced neural network to color transfer.\nSemantic segmentation BEiT v2 [1] is the latest SOTA mask modeling method published in 2022. It proposed the VQ-KD(vector-quantized knowledge distillation) method, which discretizes a continuous semantic space that supervises masked image modeling rather than relying on image pixels. This approach highly promoted the accuracy of semantic segmentation compared. The previous method that Lee et al. [3] used addressed semantic segmentation by incorporating high-order relations and mixture of label contexts into MRF [7]. They solved MRF by proposing a Convolutional Neural Network, which enables deterministic end-to-end computation in a single forward pass.\nKnowledge distillation Knowledge distillation [2] refers to transferring knowledge from a large, heavy model to a smaller model that can be practically deployed under real-world constraints. In other words, a small \u0026ldquo;student\u0026rdquo; model learns to mimic a \u0026ldquo;teacher\u0026rdquo; model and leverage the teacher\u0026rsquo;s knowledge to obtain similar or even higher accuracy.\n3. Methodology\u003e 3. Methodology # 3.1 Framework\u003e 3.1 Framework # Our framework kept the basic ideas of the Histogram Encoding Network(HEN) and Color Transfer Network(CTN) from [3]. HEN is a convolutional neural network that includes eight convolution layers and eight subsequent leaky ReLU activation of each layer. CTN is composed of a U-Net to encode and decode input images and a refinement module to process the decoding result and generate the output image.\nFigure 2: Framework of Color Transfer Network and Histogram Encoding network [3] Input image \\(I_s\\) and reference image \\(I_t\\) will be converted to LAB color space first. Histograms \\(H^l\\) for L space and \\(H^{ab}\\) for ab space of each image will then be calculated. \\(H^l\\) and \\(H^{ab}\\) will be tiled and then concatenated together as \\(H\\). The HEN will encode \\(H_s\\) and \\(H_t\\) as feature vectors \\(e_s\\) and \\(e_t\\). To fuse spatial feature maps of \\(I_s\\) and 1D global feature vectors \\(e_s\\) and \\(e_t\\), we tile the feature vectors to form histogram feature maps \\(e_s\\) and \\(e_t\\) with the spatial size of the corresponding decoding layer in the CTN to be concatenated.\nSemantic segmentation results are optional inputs of this framework. We Ô¨Årst compute local histograms from image regions with the same semantic labels and then feed the semantic-wise histograms into HEN to construct a spatially and semantically varying histogram feature map \\(R_t\\) instead of the global map \\(e_t\\). By doing this, CTN can receive more precise local information of the reference histogram, increasing the certainty in determining colors when the source and the reference image have different spatial compositions.\n3.2 Loss function\u003e 3.2 Loss function # As the authors did in [3], three types of losses are used to evaluate model performance and help with model training. Image loss \\( L_{image} \\) is the mean-squared error between the color transfer result \\(\\hat{I_t}\\) and the ground-truth image \\(I_t\\). To enforce the color transfer result \\(\\hat{I_t}\\) to have a similar histogram with the reference image, histogram loss \\(L_{hist} = MSE(\\hat{H_t}, H_t)\\) is introduced. The role of each decoder layer in CTN is to up-sample information from the previous layer, we use multi-scale loss \\(L_{multi}\\) to enforce this role. It\u0026rsquo;s defined by \\(L_{multi} = \\frac{1}{D}\\sum_{d=1}^D MSE(\\hat{I}^d_t,I_t^d)\\), where \\(D\\) is the number of layers of the decoder.\nThese losses are combined with weights \\( \\lambda_1 \\) and \\( \\lambda_2 \\):\n$$ L_{hard} = L_{image} + \\lambda_1 * L_{hist} +\\lambda_2 * L_{multi} $$\nFor knowledge distillation, we redesigned the loss to include the distance between the teacher, the pre-trained model using the methods in [3], and the student, our redesigned model with fewer parameters. The distillation loss is quite similar to the original loss, but we switched the ground-truth image \\(I_t\\) to the output of the teacher model.\nThe loss of the student model is a combination of the hard loss and the soft loss:\n$$ L = \\alpha * L_{soft} + (1-\\alpha) * L_{hard} $$\n4. Experiments\u003e 4. Experiments # 4.1 Training\u003e 4.1 Training # 4.1.1 Dataset construction\u003e 4.1.1 Dataset construction # The dataset we used is called MIT-Adobe 5K [5], which comprises the raw set of 5000 images and five same-size retouched subsets from five different experts. This dataset is excellent for our task as the source-reference images here are almost the same except for color. This would accommodate the network to learn the histogram features easily and transfer color naturally. To initiate more possible combinations of image pairs, we performed color augmentation which transfers the average hue and saturation of the source image as the authors did in [3]. Besides color augmentation, we also adopted normal image augmentation methods like random crop and horizontal flip.\n4.1.2 Implementation details\u003e 4.1.2 Implementation details # The whole network is built with the PyTorch framework, and we used 8 NVIDIA A40 GPUs to train our models. As the author only published low-performance code for model construction and testing, we built a scalable framework that supports multiple-GPU parallel training and backbone switching from scratch. Bin size when calculating histograms are \\( B_{ab}=64 \\), \\(B_{l}=8\\). We used Adam [6] as the optimizer with a learning rate of \\(5*10^{-5}\\), \\(\\beta_1 = 0.5\\), \\(\\beta_2 = 0.999\\). The weights of different losses are \\(\\lambda_1 = 10\\), \\(\\lambda_2 = 10\\), \\(\\alpha = 0.5\\).\nFigure 3: Dataset construction 4.2 Evaluation\u003e 4.2 Evaluation # The loss curves of the training process using knowledge distillation are shown in figure 4 and 5. Blue lines are the loss of the student model while pink dashed lines indicate the loss of the teacher model, which can be seen as the ultimate goal of the student model. As shown in the loss curves, it only took 25 epochs for the student model to converge, which is much faster compared with the 115 epochs the teacher model took. The student model can produce similar color transfer effects to the teacher model with slightly higher image losses and unexpected lower histogram loss.\nFigure 4: Train loss Figure 5: Validation loss 5. Results\u003e 5. Results # 5.1 Test case analysis\u003e 5.1 Test case analysis # We selected images of different categories for the test dataset, including portrait, animal, mountain, building, desert, and abstract paintings. We can get image pairs with different levels of content relevance from permutations of these images. Figures 8, 9, and 10 are examples of the color transfer results of different image pairs.\nFor strong relevance image pairs with almost identical structure and contents, figure 6 shows that the color of the sky, trees, plants, sofa, and decorations in source and reference images are matched and transferred perfectly.\nFigure 6: Results of strong relevance image pairs Weak relevance image pairs are those from the same categories with similar contents. For example, the images displayed in figure 7 all contain \u0026ldquo;desert\u0026rdquo; or \u0026ldquo;building\u0026rdquo;. However, the positions of semantic objects have less similarity than the strong ones. The output images indicate that the colors are transferred to the source images appreciably and naturally.\nFigure 7: Results of weak relevance images pairs Figure 8 shows the color transfer results between irrelevance image pairs, where the input images\u0026rsquo; content does not correlate. The two images of the first example in figure 8 are abstract digital paintings and have complex graphical shapes. The output demonstrates that our network could learn the histogram feature and transfer the primary color excellently.\nFigure 8: Irrelevance image pairs‚Äô result Besides image pairs with different content correlations, we also compared the network\u0026rsquo;s output with and without using semantic labels. The image in the bottom right corner of figure 9 is the model output without using the semantic label. It does not truly reflect all the colors contrasting with the one using the label on the left, indicating that semantic labels of the input images can help our model produce more precise color transfer results between semantically similar objects.\nFigure 9: Comparison of the results with and without using semantic label 5.2 Loss analysis\u003e 5.2 Loss analysis # We calculated the hard loss of each image pair in the training dataset, and color transfer results of the top three images with the largest and smallest loss are shown in figure 10. Each column in figure 10 represents one data point. The source, reference, and output images are in the first, second, and third rows respectively.\nFigure 10: Top 3 image pairs with the largest and smallest loss Our findings are listed below:\nImages with larger losses have more complex compositions than small ones, which means the features and colors are more challenging to be learned and transferred. Also, the semantic segmentation labels here are not precise enough to distinguish the tiny areas.\nOur network is sensitive to the angle of an image. The source image is horizontally flipped in the second column, and the people\u0026rsquo;s clothing and tree colors are misconverted.\nThe fewer different colors in the image pair, the better outcome could be produced. The three columns on the right show that image pairs with the smallest loss only contain one to two different colors.\n6. Summary\u003e 6. Summary # In this project, we built a deep-learning-based model for color transfer between images. We adopted the idea from [3] and utilized knowledge distillation to get a smaller model with similar performance to the original one, significantly reducing the training time and increasing the maximum supported input resolution. For image pairs with similar semantic objects but different compositions, we switched to BEiT v2 for generating semantic labels and greatly improved the color transfer accuracy.\nOur model can produce excellent color transfer results for image pairs with different levels of content relevance. However, although we applied the SOTA segmentation model, the semantic labels are still not accurate enough when the contents are complex, leading to unsatisfying results. The general model without using semantic labels got more natural results in these cases.\nIn the future, encoder and decoder in the Color Transfer Network with different structures, e.g. transformers, can be tried. The structure of the Color Transfer Network can also be redesigned to support high-resolution input and output better.\n7. Statement of contributions\u003e 7. Statement of contributions # Weipeng Zhang: code, dataset preparation, model training, presentation, report.\nYubing Gou: dataset preparation, model testing and analysis, report.\n[1] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022. \u003e [1] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022. # [2] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.\u003e [2] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. # [3] Junyong Lee, Hyeongseok Son, Gunhee Lee, Jonghyeop Lee, Sunghyun Cho, and Seungyong Lee. Deep color transfer using histogram analogy. The Visual Computer, 36(10):2129‚Äì2143, 2020. \u003e [3] Junyong Lee, Hyeongseok Son, Gunhee Lee, Jonghyeop Lee, Sunghyun Cho, and Seungyong Lee. Deep color transfer using histogram analogy. The Visual Computer, 36(10):2129‚Äì2143, 2020. # [4] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4990‚Äì4998, 2017.\u003e [4] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4990‚Äì4998, 2017. # [5] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr ÃÅedo Durand. Learning photographic global tonal adjustment with a database of input / output image pairs. In The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition, 2011.\u003e [5] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr ÃÅedo Durand. Learning photographic global tonal adjustment with a database of input / output image pairs. In The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition, 2011. # [6] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\u003e [6] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. # [7] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and Xiaoou Tang. Deep learning markov random field for semantic segmentation. IEEE transactions on pattern analysis and machine intelligence, 40(8):1814‚Äì1828, 2017.\u003e [7] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and Xiaoou Tang. Deep learning markov random field for semantic segmentation. IEEE transactions on pattern analysis and machine intelligence, 40(8):1814‚Äì1828, 2017. # ","date":"5 December 2022","permalink":"/Deep-Color-Transfer/posts/report/","section":"Posts","summary":"Deep color transfer is an approach to extracting the histogram containing color information from the source and the reference image, then transferring the color between the image pair based on the histogram using a deep neural network.","title":"Final Report"},{"content":"","date":"5 December 2022","permalink":"/Deep-Color-Transfer/posts/","section":"Posts","summary":"","title":"Posts"},{"content":" Conceptual Review\u003e Conceptual Review # The authors of Deep Color Transfer Using Histogram Analogy [1] proposed a deep learning framework that leverages color histogram analogy for color transfer between source and reference images. The framework consists of two networks, a Histogram Encoding Network (HEN) and a Color Transfer Network (CTN) (Fig. 1). HEN extracts encoded information from the histograms of source and reference images, which is fed into CTN to guide the color transfer process. Although a histogram is a simple and global representation of image colors, convolutional neural networks with encoded histograms can conditionally transfer the colors of reference images to source images. For strongly relevant and irrelevant cases, the same histogram information is used for all parts of the source image, and this is the default setting. When semantic object information is important, as in the case of weak relevance, semantic image segmentation is used and the histogram analogy is extracted and applied for corresponding semantic regions between source and reference images.\nFig. 1 Network architecture. Implementation Details\u003e Implementation Details # As the authors did in [1], we‚Äôll train our model on a paired dataset constructed from the MIT-Adobe 5K dataset [2] which consists of five sets, each of which contains 5,000 images. Since using the dataset only provides image pairs with a fixed number of combinations, we also follow the authors to perform color augmentation by transforming the average hue and saturation of the original images to produce more diverse image pairs. In addition, as an ideally trained model must produce the output image the same as the source image if the histograms of the two images are the same, we used identical source and reference image pairs to stabilize the network output. We implement our network on Pytorch. We use the Adam optimizer with the fixed learning rate \\(5*10^{-5}\\), \\(\\beta_1=0.5\\), and \\(\\beta_2=0.999\\). As we jointly train HEN and CTN, the objective function is \\(L_{total} = L_{image} + \\lambda_1 L_{hist} + \\lambda_2 L_{multi}\\), where \\(L_{image}\\), \\(L_{hist}\\) and \\(L_{multi}\\) are a image loss, a histogram loss, and a multi-scale loss. We used \\(\\lambda_1=1.5\\) and \\(\\lambda_2=0.5\\). We train the network for 100 epochs.\nFindings\u003e Findings # As the image semantic segmentation map may have a huge impact on the final output of our model, we tried several segmentaion models and finalized BEiT v2 [3] as our segmentation model. We built the model and loaded downloaded pre-trained weights and then generated the segmentation maps. As shown in Fig 2, the segmentation result from BEiT v2 is quite accurate.\nFig. 2 BEiT v2 generated segmentation map. With accurate segmentation maps generated, we tested 4 sets of image pairs and the result is showed in Fig 3. When the reference image and input image have similar semantic content, the model can generate a great color transfer without being fed with segmentation maps. However, when there\u0026rsquo;s a huge content difference, segmentation maps can help improve the model performance (see row 3). The model works well even when the reference image is just a color palette.\nFig. 3 Experiment results on different cases. Plans\u003e Plans # The experiment we did above meets our expection. In the next few weeks, we\u0026rsquo;ll try to change the structure of our current model and see if better results can be generated. The size of our model is relatively large right now, we\u0026rsquo;ll also try some knowledge distillation techniques and smaller model structure like MobileNet.\n[1] Lee, Junyong, et al. \u0026ldquo;Deep color transfer using histogram analogy.\u0026rdquo; The Visual Computer 36.10 (2020): 2129-2143.\u003e [1] Lee, Junyong, et al. \u0026ldquo;Deep color transfer using histogram analogy.\u0026rdquo; The Visual Computer 36.10 (2020): 2129-2143. # [2] Bychkovsky, Vladimir, et al. \u0026ldquo;Learning photographic global tonal adjustment with a database of input/output image pairs.\u0026rdquo; CVPR 2011. IEEE, 2011.\u003e [2] Bychkovsky, Vladimir, et al. \u0026ldquo;Learning photographic global tonal adjustment with a database of input/output image pairs.\u0026rdquo; CVPR 2011. IEEE, 2011. # [3] Peng, Zhiliang, et al. \u0026ldquo;Beit v2: Masked image modeling with vector-quantized visual tokenizers.\u0026rdquo; arXiv preprint arXiv:2208.06366 (2022).\u003e [3] Peng, Zhiliang, et al. \u0026ldquo;Beit v2: Masked image modeling with vector-quantized visual tokenizers.\u0026rdquo; arXiv preprint arXiv:2208.06366 (2022). # ","date":"23 October 2022","permalink":"/Deep-Color-Transfer/posts/progress/","section":"Posts","summary":"Conceptual Review\u003e Conceptual Review # The authors of Deep Color Transfer Using Histogram Analogy [1] proposed a deep learning framework that leverages color histogram analogy for color transfer between source and reference images.","title":"Progress Report"},{"content":"In this project, we are trying to build a deep learning model for color transfer between images.\nEarly color transfer approaches change colors of a source image based on the global color distribution of a reference image. Some methods perform color transfer by matching the means and standard deviations of source to reference images while others explore color mapping between source and reference images leveraging probability density functions of colors. These conventional approaches, however, may fail to reflect semantic correspondence because they do not consider any spatial information. To handle the limitation of global color transfer, several approaches contemplate local correspondences between source and reference images. While these approaches solve the problem of global color transfer in the case that source and reference images have similar structures, they may fail when both images have totally irrelevant contents and styles.\nFig. 1 Demo of Deep Color Transfer Using Histogram Analogy. The authors of Deep Color Transfer Using Histogram Analogy [1] proposed a deep learning framework that leverages color histogram analogy for color transfer between source and reference images (Fig. 1). The framework consists of two networks, a Histogram Encoding Network (HEN) and a Color Transfer Network (CTN) (Fig. 2). HEN extracts encoded information from the histograms of source and reference images, which is fed into CTN to guide the color transfer process. Although a histogram is a simple and global representation of image colors, convolutional neural networks with encoded histograms can conditionally transfer the colors of reference images to source images. For strongly relevant and irrelevant cases, the same histogram information is used for all parts of the source image, and this is the default setting. When semantic object information is important, as in the case of weak relevance, semantic image segmentation is used and the histogram analogy is extracted and applied for corresponding semantic regions between source and reference images.\nFig. 2 Network architecture. As the authors did in [1], we‚Äôll train our model on a paired dataset constructed from the MIT-Adobe 5K dataset [2] which consists of six sets, each of which contains 5,000 images. In the dataset, the first set contains the original images and the other five sets contain color varied images of the original images retouched by five different experts. The paired dataset enables the model to learn to transfer color into natural-looking images. Since using the dataset only provides image pairs with a fixed number of combinations, we‚Äôll also perform color augmentation by transforming the average hue and saturation of the original images to produce more diverse image pairs. In addition, as an ideally trained model must produce the output image the same as the source image if the histograms of the two images are the same, we‚Äôll use identical source and reference image pairs to stabilize the network output.\nThe paper this project bases on was published in 2020 and it has been 2 years since that, during which many advanced vision models were proposed and have brought the scores of benchmarks to a new level. Beyond implementing the model mentioned in [1], we will try some SOTA semantic(or panoptic) segmentation methods to see if they could improve the performance in the case of weak relevance between the source image and the reference image. Image transformers like BEiT [3], Swin Transformers [4], and MaX-DeepLab [5] have drawn much attention recently, we will try to introduce transformers into the current framework and see if a better color transfer can be produced.\n[1] Lee, Junyong, et al. \u0026ldquo;Deep color transfer using histogram analogy.\u0026rdquo; The Visual Computer 36.10 (2020): 2129-2143.\u003e [1] Lee, Junyong, et al. \u0026ldquo;Deep color transfer using histogram analogy.\u0026rdquo; The Visual Computer 36.10 (2020): 2129-2143. # [2] Bychkovsky, Vladimir, et al. \u0026ldquo;Learning photographic global tonal adjustment with a database of input/output image pairs.\u0026rdquo; CVPR 2011. IEEE, 2011.\u003e [2] Bychkovsky, Vladimir, et al. \u0026ldquo;Learning photographic global tonal adjustment with a database of input/output image pairs.\u0026rdquo; CVPR 2011. IEEE, 2011. # [3] Bao, Hangbo, Li Dong, and Furu Wei. \u0026ldquo;Beit: Bert pre-training of image transformers.\u0026rdquo; arXiv preprint arXiv:2106.08254 (2021).\u003e [3] Bao, Hangbo, Li Dong, and Furu Wei. \u0026ldquo;Beit: Bert pre-training of image transformers.\u0026rdquo; arXiv preprint arXiv:2106.08254 (2021). # [4] Liu, Ze, et al. \u0026ldquo;Swin transformer: Hierarchical vision transformer using shifted windows.\u0026rdquo; Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\u003e [4] Liu, Ze, et al. \u0026ldquo;Swin transformer: Hierarchical vision transformer using shifted windows.\u0026rdquo; Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. # [5] Wang, Huiyu, et al. \u0026ldquo;Max-deeplab: End-to-end panoptic segmentation with mask transformers.\u0026rdquo; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\u003e [5] Wang, Huiyu, et al. \u0026ldquo;Max-deeplab: End-to-end panoptic segmentation with mask transformers.\u0026rdquo; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. # ","date":"18 October 2022","permalink":"/Deep-Color-Transfer/posts/proposal/","section":"Posts","summary":"In this project, we are trying to build a deep learning model for color transfer between images.\nEarly color transfer approaches change colors of a source image based on the global color distribution of a reference image.","title":"Project Proposal"},{"content":"","date":"1 January 0001","permalink":"/Deep-Color-Transfer/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/Deep-Color-Transfer/tags/","section":"Tags","summary":"","title":"Tags"}]